{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sampling distribution: z~N(O, I)\n",
    "- posterior model: g_nu(z) = theta\n",
    "- expected reward model: f_theta(action) = E(y|action, theta)\n",
    "- noise for observations: sigmao\n",
    "- prior std: sigmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KBandits:\n",
    "    def __init__(self, k, sigma_obs=0.5, sigma_model=2):\n",
    "        self.rewards = np.random.randn(k) * sigma_model\n",
    "        self.n_bandits = k\n",
    "        self.sigma_obs = sigma_obs\n",
    "        self.sigma_model = sigma_model\n",
    "    \n",
    "    def reset(self):\n",
    "        self.rewards = np.random.randn(k) * self.sigma_model\n",
    "    \n",
    "    def set_model(self, theta):\n",
    "        self.rewards = theta\n",
    "    \n",
    "    def step(self, action):\n",
    "        return (np.random.randn(self.n_bandits) * self.sigma_obs + self.rewards)[action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_dataset(env, npoints, nbandits):\n",
    "    \"\"\"\n",
    "    output of shape (Npoints, Kbandits)\n",
    "    \"\"\"\n",
    "    return np.array([[env.step(i) for i in range(nbandits)] for _ in range(npoints)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 3.85540057, -3.46561015,  4.00836241, -3.41760425]),\n",
       " array([0.49310461, 0.50771701, 0.5113953 , 0.51589582]))"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = gather_dataset(KBandits(4), 1000, 4)\n",
    "d.mean(0), d.std(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_selection_mapping(theta, x):\n",
    "    \"\"\"\n",
    "    theta of size (B, K)\n",
    "    x of size (B,)\n",
    "    \"\"\"\n",
    "    return torch.gather(theta, dim=1, index=x.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearHypermodelBandits:\n",
    "    def __init__(self, hpm_size, sigma_prior, k_arms, device='cpu'):\n",
    "        self.hpm_size = hpm_size\n",
    "        self.sigma_p = sigma_prior\n",
    "        self.n_arms = k_arms\n",
    "        self.posterior_model_g = LinearModuleBandits(k_bandits=k_arms, model_dim=hpm_size)\n",
    "        self.device = device\n",
    "        self.posterior_model_g = self.posterior_model_g.to(device)\n",
    "        self.prior = self.sample_prior_dbz() # of shape (k, )\n",
    "        \n",
    "    \n",
    "    def sample_prior_dbz(self):\n",
    "        D = np.random.randn(self.n_arms) * self.sigma_p # dim (k, k)\n",
    "        B = generate_hypersphere(dim=self.hpm_size, n_samples=self.n_arms, norm=1) # dim (k, m)\n",
    "        z = np.random.randn(self.n_arms, self.hpm_size) # dim (k, m)\n",
    "        return torch.from_numpy(D * (B * z).sum(-1)).to(self.device)\n",
    "    \n",
    "    def sample_posterior(self, n_samples):\n",
    "        return self.posterior_model_g.sample(n_samples) + self.prior # NEED TO DISCUSS THIS\n",
    "    \n",
    "    def update_device(device):\n",
    "        self.device = device\n",
    "        self.prior_posterior_model_g = self.prior_posterior_model_g.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm = LinearHypermodelBandits(hpm_size=1, sigma_prior=0.5, k_arms=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = np.array([hm.sample_prior_dbz().numpy() for _ in range(1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.01785005,  0.0320669 ,  0.01765375, -0.02198162]),\n",
       " array([0.52112837, 0.50227694, 0.46088904, 0.49820453]))"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.mean(0), data_test.std(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModuleBandits(nn.Module):\n",
    "    def __init__(self, k_bandits, model_dim):\n",
    "        super(LinearModuleBandits, self).__init__()\n",
    "        self.k = k_bandits\n",
    "        self.m = model_dim\n",
    "        self.C = nn.Parameter(torch.ones(size=(k_bandits, model_dim)))\n",
    "        self.mu = nn.Parameter(torch.ones(k_bandits))\n",
    "        self.init_parameters()\n",
    "    \n",
    "    def init_parameters(self):\n",
    "        mu_sampled = torch.randn(self.k) * 0.05\n",
    "        c_sampled = torch.randn(self.k, self.m) * 0.05\n",
    "        self.C.data = c_sampled\n",
    "        self.mu.data = mu_sampled\n",
    "    \n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        z of size (batch, Kbandits, modelsize)\n",
    "        theta of size (batch, Kbandits)\n",
    "        \"\"\"\n",
    "        return (self.C * z).sum(-1) + self.mu\n",
    "    \n",
    "    def sample(self, n_samples):\n",
    "        return self.forward(torch.randn(n_samples, self.k, self.m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0327, -0.0407,  0.0043,  0.0232], grad_fn=<MeanBackward1>),\n",
       " tensor([0.0157, 0.0130, 0.0677, 0.0580], grad_fn=<StdBackward1>))"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = hm.posterior_model_g.sample(10000)\n",
    "data_test.mean(0), data_test.std(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4995,  0.5636,  0.4819,  0.1375], dtype=torch.float64)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hm.prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0327, -0.0409,  0.0021,  0.0230])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hm.posterior_model_g.mu.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.4667,  0.5226,  0.4842,  0.1604], dtype=torch.float64,\n",
       "        grad_fn=<MeanBackward1>),\n",
       " tensor([0.0157, 0.0130, 0.0690, 0.0585], dtype=torch.float64,\n",
       "        grad_fn=<StdBackward1>))"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = hm.sample_posterior(10000)\n",
    "data_test.mean(0), data_test.std(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hypersphere(dim, n_samples, norm=1):\n",
    "    if norm==1: # TODO ask question about that\n",
    "        samples = np.random.rand(n_samples, dim)\n",
    "        samples = samples / np.expand_dims(np.abs(samples).sum(1), 1)\n",
    "        return samples\n",
    "    elif norm==2:\n",
    "        samples = np.random.randn(n_samples, dim)\n",
    "        samples = samples / np.expand_dims(np.sqrt((samples ** 2).sum(1)), 1)\n",
    "        return samples\n",
    "    else:\n",
    "        raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    def __init__(self, k_bandits):\n",
    "        self.n_arms = k_bandits\n",
    "\n",
    "    def act(self):\n",
    "        return np.random.randint(self.n_arms)\n",
    "\n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmented_dataset(dataset, perturbations_dimension, mode='hypersphere'):\n",
    "    n_points = len(dataset)\n",
    "    if mode == 'hypersphere':\n",
    "        perturbations = generate_hypersphere(dim=perturbations_dimension, n_samples=n_points, norm=1)\n",
    "    else:\n",
    "        perturbations = generate_hypersphere(dim=perturbations_dimension, n_samples=n_points, norm=2)\n",
    "    return [tuple([*data_point, perturbations[ix, :]]) for ix, data_point in enumerate(dataset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(envnmt, actor, horizon, n_steps, n_samples_z, lr, sigmao, sigmap, batch_size, hypermodel, update_every=1, training=False, device='cpu'):\n",
    "    obs = []\n",
    "    new_data = []\n",
    "    dataset = []\n",
    "    for ix in range(horizon):\n",
    "        arm_selected = actor.act()\n",
    "        reward = envnmt.step(arm_selected)\n",
    "        data_point = [arm_selected, reward]\n",
    "        obs.append(data_point)\n",
    "        new_data.append(data_point)\n",
    "        if (ix + 1) % update_every == 0:\n",
    "            dataset += augmented_dataset(new_data, perturbations_dimension=hypermodel.hpm_size)\n",
    "            new_data = []\n",
    "            if training:\n",
    "                data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "                train_hypermodel(data_loader,\n",
    "                                 nsteps=n_steps,\n",
    "                                 nsamples_z=n_samples_z,\n",
    "                                 learning_rate=lr,\n",
    "                                 hypermodel=hypmodel,\n",
    "                                 sigmao=sigmao,\n",
    "                                 sigmap=sigmap,\n",
    "                                 device='cpu')\n",
    "    return obs, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hypermodel(data_loader, hypermodel, nsteps, nsamples_z, learning_rate, sigmao, sigmap, device='cpu'):\n",
    "    optimizer = SGD(hypermodel.posterior_model_g.parameters(), lr=learning_rate, weight_decay=1/(2 * sigmap ** 2))\n",
    "    steps_done = 0\n",
    "    total_loss = 0\n",
    "    while True:\n",
    "        for batch in data_loader:\n",
    "            x, y, a = batch\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            a = a.to(device) # shape B, m\n",
    "#             import ipdb;\n",
    "#             ipdb.set_trace()\n",
    "            z_sample = torch.randn(nsamples_z, k, modelling_size, device=device) # shape M, K, m\n",
    "            z_sliced = torch.index_select(z_sample, 1, x) # shape M, B, m\n",
    "            \n",
    "            sigAz = sigmao * (a.unsqueeze(0) * z_sliced).sum(-1) #shape M, B\n",
    "            \n",
    "            posteriors = hypermodel.posterior_model_g(z_sample)\n",
    "            outputs = torch.index_select(posteriors, 1, x)\n",
    "            \n",
    "            loss = (((y + sigAz - outputs) ** 2).mean(1)).mean(0) #/ (2 * sigmao ** 2)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            steps_done += 1\n",
    "            if (steps_done % 25) == 0:\n",
    "                print(f\"step {steps_done}, loss:{loss.item():2f}\")\n",
    "            if steps_done >= nsteps:\n",
    "                return total_loss / nsteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 1000\n",
    "sigmao = 0.1 # environmnet parameters\n",
    "sigmap = 1.\n",
    "\n",
    "sigmap_algo = 0.5 # hypermodel prior width\n",
    "\n",
    "sigmap_training = 10. # weight decay penalty\n",
    "sigmao_training = 0.5\n",
    "updates_freq = 1\n",
    "batch_s = 16\n",
    "lr = 5 * 1e-2\n",
    "n_samples_z = 16\n",
    "n_steps = 1000\n",
    "\n",
    "k = 10\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "modelling_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = KBandits(k=k,\n",
    "               sigma_obs=sigmao,\n",
    "               sigma_model=sigmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.1515557 ,  1.99217456,  0.80892846, -0.31244675,  0.70451408,\n",
       "       -1.70928757,  1.73128574,  0.69615721, -0.38798008,  1.10477927])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.15695174,  1.98961239,  0.80447679, -0.30946026,  0.70149627,\n",
       "        -1.71306783,  1.73528717,  0.69431881, -0.38710757,  1.10509069]),\n",
       " array([0.10305578, 0.09509185, 0.09925482, 0.09750421, 0.09746322,\n",
       "        0.09762265, 0.09930263, 0.10072955, 0.0996663 , 0.10275035]))"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = gather_dataset(env, 1000, k)\n",
    "d.mean(0), d.std(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypmodel = LinearHypermodelBandits(hpm_size=modelling_size,\n",
    "                                   sigma_prior=sigmap_algo,\n",
    "                                   k_arms=k,\n",
    "                                   device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0161,  0.0008,  0.0006,  0.0803,  0.1147, -0.0117,  0.0608, -0.0809,\n",
       "         -0.0086, -0.0476], grad_fn=<MeanBackward1>),\n",
       " tensor([0.0804, 0.0731, 0.0782, 0.0206, 0.0655, 0.0491, 0.0521, 0.0174, 0.0582,\n",
       "         0.0413], grad_fn=<StdBackward1>))"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = hypmodel.posterior_model_g.sample(10000)\n",
    "data_test.mean(0), data_test.std(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3166,  0.1244,  0.3018,  0.0734,  0.0136, -0.3415,  0.4828,  0.3423,\n",
       "        -0.0254, -0.1283], dtype=torch.float64)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypmodel.prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0161,  0.0015,  0.0006,  0.0803,  0.1138, -0.0115,  0.0610, -0.0808,\n",
       "        -0.0092, -0.0470])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypmodel.posterior_model_g.mu.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.4643,  2.0717,  1.0991, -0.2398,  0.6913, -2.0047,  2.1761,  0.9925,\n",
       "         -0.3997,  0.9433], dtype=torch.float64, grad_fn=<MeanBackward1>),\n",
       " tensor([0.3428, 0.3535, 0.3466, 0.3470, 0.3465, 0.3419, 0.3555, 0.3449, 0.3467,\n",
       "         0.3464], dtype=torch.float64, grad_fn=<StdBackward1>))"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = hypmodel.sample_posterior(10000)\n",
    "data_test.mean(0), data_test.std(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.1515557 ,  1.99217456,  0.80892846, -0.31244675,  0.70451408,\n",
       "       -1.70928757,  1.73128574,  0.69615721, -0.38798008,  1.10477927])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, dataset = run_episode(envnmt=env,\n",
    "                           actor=RandomAgent(k_bandits=k),\n",
    "                           horizon=H,\n",
    "                           n_steps=n_steps,\n",
    "                           n_samples_z=n_samples_z,\n",
    "                           lr=lr,\n",
    "                           sigmao = sigmao_algo,\n",
    "                           sigmap = sigmap_algo,\n",
    "                           batch_size=batch_s,\n",
    "                           update_every=1,\n",
    "                           hypermodel=hypmodel,\n",
    "                           training=False,\n",
    "                           device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(dataset, batch_size=batch_s, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 25, loss:1.160254\n",
      "step 50, loss:0.557353\n",
      "step 75, loss:0.309358\n",
      "step 100, loss:0.189499\n",
      "step 125, loss:0.212733\n",
      "step 150, loss:0.141650\n",
      "step 175, loss:0.113221\n",
      "step 200, loss:0.081919\n",
      "step 225, loss:0.062137\n",
      "step 250, loss:0.037872\n",
      "step 275, loss:0.047653\n",
      "step 300, loss:0.039998\n",
      "step 325, loss:0.035347\n",
      "step 350, loss:0.048748\n",
      "step 375, loss:0.028004\n",
      "step 400, loss:0.041591\n",
      "step 425, loss:0.023608\n",
      "step 450, loss:0.036558\n",
      "step 475, loss:0.029538\n",
      "step 500, loss:0.037406\n",
      "step 525, loss:0.029133\n",
      "step 550, loss:0.037377\n",
      "step 575, loss:0.032833\n",
      "step 600, loss:0.037851\n",
      "step 625, loss:0.030647\n",
      "step 650, loss:0.023506\n",
      "step 675, loss:0.032300\n",
      "step 700, loss:0.029240\n",
      "step 725, loss:0.034913\n",
      "step 750, loss:0.029136\n",
      "step 775, loss:0.035578\n",
      "step 800, loss:0.027723\n",
      "step 825, loss:0.040517\n",
      "step 850, loss:0.032273\n",
      "step 875, loss:0.024940\n",
      "step 900, loss:0.028806\n",
      "step 925, loss:0.049389\n",
      "step 950, loss:0.032057\n",
      "step 975, loss:0.045163\n",
      "step 1000, loss:0.035307\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.11077446661083155"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_hypermodel(data_loader=dl,\n",
    "                 nsteps=n_steps, \n",
    "                 nsamples_z=n_samples_z,\n",
    "                 learning_rate=lr,\n",
    "                 sigmao=sigmao_training,\n",
    "                 sigmap=sigmap_training,\n",
    "                 hypermodel=hypmodel, device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODOs:\n",
    "- online loop with TS\n",
    "- similar offline tests with the MNL bandit env\n",
    "- script for experiments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
