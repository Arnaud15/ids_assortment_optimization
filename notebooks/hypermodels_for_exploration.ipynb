{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sampling distribution: z~N(O, I)\n",
    "- posterior model: g_nu(z) = theta\n",
    "- expected reward model: f_theta(action) = E(y|action, theta)\n",
    "- noise for observations: sigmao\n",
    "- prior std: sigmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_dataset(env, npoints, nbandits):\n",
    "    \"\"\"\n",
    "    output of shape (Npoints, Kbandits)\n",
    "    \"\"\"\n",
    "    return np.array([[env.step(i) for i in range(nbandits)] for _ in range(npoints)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.6109008 , -0.06494404,  0.1155856 ,  0.40743141]),\n",
       " array([0.49614104, 0.50321473, 0.49427322, 0.49183457]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = gather_dataset(KBandits(4), 1000, 4)\n",
    "d.mean(0), d.std(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_selection_mapping(theta, x):\n",
    "    \"\"\"\n",
    "    theta of size (B, K)\n",
    "    x of size (B,)\n",
    "    \"\"\"\n",
    "    return torch.gather(theta, dim=1, index=x.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearHypermodelBandits:\n",
    "    def __init__(self, hpm_size, sigma_prior, k_arms, device='cpu'):\n",
    "        self.hpm_size = hpm_size\n",
    "        self.sigma_p = sigma_prior\n",
    "        self.n_arms = k_arms\n",
    "        self.posterior_model_g = LinearModuleBandits(k_bandits=k_arms, model_dim=hpm_size)\n",
    "        self.device = device\n",
    "        self.posterior_model_g = self.posterior_model_g.to(device)\n",
    "        self.prior = self.sample_prior_dbz() # of shape (k, )\n",
    "        \n",
    "    \n",
    "    def sample_prior_dbz(self):\n",
    "        D = np.random.randn(self.n_arms) * self.sigma_p # dim (k, k)\n",
    "        B = generate_hypersphere(dim=self.hpm_size, n_samples=self.n_arms, norm=1) # dim (k, m)\n",
    "        z = np.random.randn(self.n_arms, self.hpm_size) # dim (k, m)\n",
    "        return torch.from_numpy(D * (B * z).sum(-1)).to(self.device)\n",
    "    \n",
    "    def sample_posterior(self, n_samples):\n",
    "        return self.posterior_model_g.sample(n_samples) + self.prior # NEED TO DISCUSS THIS\n",
    "    \n",
    "    def update_device(device):\n",
    "        self.device = device\n",
    "        self.prior_posterior_model_g = self.prior_posterior_model_g.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm = LinearHypermodelBandits(hpm_size=1, sigma_prior=0.5, k_arms=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = np.array([hm.sample_prior_dbz().numpy() for _ in range(1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.00923772,  0.00077974, -0.00522484,  0.02766258]),\n",
       " array([0.46525   , 0.50226105, 0.48135034, 0.49806974]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.mean(0), data_test.std(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModuleBandits(nn.Module):\n",
    "    def __init__(self, k_bandits, model_dim):\n",
    "        super(LinearModuleBandits, self).__init__()\n",
    "        self.k = k_bandits\n",
    "        self.m = model_dim\n",
    "        self.C = nn.Parameter(torch.ones(size=(k_bandits, model_dim)))\n",
    "        self.mu = nn.Parameter(torch.ones(k_bandits))\n",
    "        self.init_parameters()\n",
    "    \n",
    "    def init_parameters(self):\n",
    "        mu_sampled = torch.randn(self.k) * 0.05\n",
    "        c_sampled = torch.randn(self.k, self.m) * 0.05\n",
    "        self.C.data = c_sampled\n",
    "        self.mu.data = mu_sampled\n",
    "    \n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        z of size (batch, Kbandits, modelsize)\n",
    "        theta of size (batch, Kbandits)\n",
    "        \"\"\"\n",
    "        return (self.C * z).sum(-1) + self.mu\n",
    "    \n",
    "    def sample(self, n_samples):\n",
    "        return self.forward(torch.randn(n_samples, self.k, self.m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0049, -0.0116,  0.0170,  0.0108], grad_fn=<MeanBackward1>),\n",
       " tensor([0.0751, 0.0433, 0.0542, 0.0729], grad_fn=<StdBackward1>))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = hm.posterior_model_g.sample(10000)\n",
    "data_test.mean(0), data_test.std(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0279,  0.0189,  0.2632,  0.0967], dtype=torch.float64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hm.prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0051, -0.0109,  0.0166,  0.0106])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hm.posterior_model_g.mu.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0219,  0.0084,  0.2799,  0.1084], dtype=torch.float64,\n",
       "        grad_fn=<MeanBackward1>),\n",
       " tensor([0.0738, 0.0435, 0.0538, 0.0727], dtype=torch.float64,\n",
       "        grad_fn=<StdBackward1>))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = hm.sample_posterior(10000)\n",
    "data_test.mean(0), data_test.std(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hypersphere(dim, n_samples, norm=1):\n",
    "    if norm==1: # TODO ask question about that\n",
    "        samples = np.random.rand(n_samples, dim)\n",
    "        samples = samples / np.expand_dims(np.abs(samples).sum(1), 1)\n",
    "        return samples\n",
    "    elif norm==2:\n",
    "        samples = np.random.randn(n_samples, dim)\n",
    "        samples = samples / np.expand_dims(np.sqrt((samples ** 2).sum(1)), 1)\n",
    "        return samples\n",
    "    else:\n",
    "        raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    def __init__(self, k_bandits):\n",
    "        self.n_arms = k_bandits\n",
    "\n",
    "    def act(self):\n",
    "        return np.random.randint(self.n_arms)\n",
    "\n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmented_dataset(dataset, perturbations_dimension, mode='hypersphere'):\n",
    "    n_points = len(dataset)\n",
    "    if mode == 'hypersphere':\n",
    "        perturbations = generate_hypersphere(dim=perturbations_dimension, n_samples=n_points, norm=1)\n",
    "    else:\n",
    "        perturbations = generate_hypersphere(dim=perturbations_dimension, n_samples=n_points, norm=2)\n",
    "    return [tuple([*data_point, perturbations[ix, :]]) for ix, data_point in enumerate(dataset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(envnmt, actor, horizon, n_steps, n_samples_z, lr, sigmao, sigmap, batch_size, hypermodel, update_every=1, training=False, device='cpu'):\n",
    "    obs = []\n",
    "    new_data = []\n",
    "    dataset = []\n",
    "    for ix in range(horizon):\n",
    "        arm_selected = actor.act()\n",
    "        reward = envnmt.step(arm_selected)\n",
    "        data_point = [arm_selected, reward]\n",
    "        obs.append(data_point)\n",
    "        new_data.append(data_point)\n",
    "        if (ix + 1) % update_every == 0:\n",
    "            dataset += augmented_dataset(new_data, perturbations_dimension=hypermodel.hpm_size)\n",
    "            new_data = []\n",
    "            if training:\n",
    "                data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "                train_hypermodel(data_loader,\n",
    "                                 nsteps=n_steps,\n",
    "                                 nsamples_z=n_samples_z,\n",
    "                                 learning_rate=lr,\n",
    "                                 hypermodel=hypmodel,\n",
    "                                 sigmao=sigmao,\n",
    "                                 sigmap=sigmap,\n",
    "                                 device='cpu')\n",
    "    return obs, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hypermodel(data_loader, hypermodel, nsteps, nsamples_z, learning_rate, sigmao, sigmap, device='cpu'):\n",
    "    optimizer = SGD(hypermodel.posterior_model_g.parameters(), lr=learning_rate, weight_decay=1/(2 * sigmap ** 2))\n",
    "    steps_done = 0\n",
    "    total_loss = 0\n",
    "    while True:\n",
    "        for batch in data_loader:\n",
    "            x, y, a = batch\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            a = a.to(device) # shape B, m\n",
    "#             import ipdb;\n",
    "#             ipdb.set_trace()\n",
    "            z_sample = torch.randn(nsamples_z, k, modelling_size, device=device) # shape M, K, m\n",
    "            z_sliced = torch.index_select(z_sample, 1, x) # shape M, B, m\n",
    "            \n",
    "            sigAz = sigmao * (a.unsqueeze(0) * z_sliced).sum(-1) #shape M, B\n",
    "            \n",
    "            posteriors = hypermodel.posterior_model_g(z_sample)\n",
    "            outputs = torch.index_select(posteriors, 1, x)\n",
    "            \n",
    "            loss = (((y + sigAz - outputs) ** 2).mean(1)).mean(0) #/ (2 * sigmao ** 2)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            steps_done += 1\n",
    "            if (steps_done % 25) == 0:\n",
    "                print(f\"step {steps_done}, loss:{loss.item():2f}\")\n",
    "            if steps_done >= nsteps:\n",
    "                return total_loss / nsteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 1000\n",
    "sigmao = 0.1 # environmnet parameters\n",
    "sigmap = 1.\n",
    "\n",
    "sigmap_algo = 0.5 # hypermodel prior width\n",
    "\n",
    "sigmap_training = 10. # weight decay penalty\n",
    "sigmao_training = 0.5\n",
    "updates_freq = 1\n",
    "batch_s = 16\n",
    "lr = 5 * 1e-2\n",
    "n_samples_z = 16\n",
    "n_steps = 1000\n",
    "\n",
    "k = 10\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "modelling_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = KBandits(k=k,\n",
    "               sigma_obs=sigmao,\n",
    "               sigma_model=sigmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.37918959, -0.69634992,  0.72583669, -0.92180339,  0.38468225,\n",
       "       -0.24488638,  0.40237648,  0.36897392, -0.01279609, -0.21216963])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.38412976, -0.69290271,  0.72406354, -0.92475952,  0.38947336,\n",
       "        -0.24202389,  0.39885424,  0.37386838, -0.00576701, -0.2126568 ]),\n",
       " array([0.09937621, 0.09730512, 0.10095172, 0.09576024, 0.09575365,\n",
       "        0.09955051, 0.09890657, 0.10147648, 0.09624598, 0.10038414]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = gather_dataset(env, 1000, k)\n",
    "d.mean(0), d.std(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypmodel = LinearHypermodelBandits(hpm_size=modelling_size,\n",
    "                                   sigma_prior=sigmap_algo,\n",
    "                                   k_arms=k,\n",
    "                                   device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.3528, -0.6733,  0.7074, -0.8959,  0.3708, -0.2315,  0.4133,  0.3414,\n",
       "         -0.0192, -0.2151], grad_fn=<MeanBackward1>),\n",
       " tensor([0.3452, 0.3431, 0.3481, 0.3429, 0.3458, 0.3482, 0.3461, 0.3516, 0.3437,\n",
       "         0.3524], grad_fn=<StdBackward1>))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = hypmodel.posterior_model_g.sample(10000)\n",
    "data_test.mean(0), data_test.std(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0261, -0.6305, -0.0487, -0.0297, -0.2935, -0.3449,  0.7815,  0.4746,\n",
       "         0.0347,  0.6643], dtype=torch.float64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypmodel.prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3591, -0.6776,  0.7099, -0.9018,  0.3716, -0.2324,  0.4161,  0.3399,\n",
       "        -0.0202, -0.2139])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypmodel.posterior_model_g.mu.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.3339, -1.3027,  0.6554, -0.9324,  0.0847, -0.5825,  1.1973,  0.8038,\n",
       "          0.0187,  0.4500], dtype=torch.float64, grad_fn=<MeanBackward1>),\n",
       " tensor([0.3389, 0.3480, 0.3473, 0.3455, 0.3422, 0.3466, 0.3337, 0.3550, 0.3440,\n",
       "         0.3537], dtype=torch.float64, grad_fn=<StdBackward1>))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = hypmodel.sample_posterior(10000)\n",
    "data_test.mean(0), data_test.std(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.37918959, -0.69634992,  0.72583669, -0.92180339,  0.38468225,\n",
       "       -0.24488638,  0.40237648,  0.36897392, -0.01279609, -0.21216963])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, dataset = run_episode(envnmt=env,\n",
    "                           actor=RandomAgent(k_bandits=k),\n",
    "                           horizon=H,\n",
    "                           n_steps=n_steps,\n",
    "                           n_samples_z=n_samples_z,\n",
    "                           lr=lr,\n",
    "                           sigmao = sigmao_training,\n",
    "                           sigmap = sigmap_training,\n",
    "                           batch_size=batch_s,\n",
    "                           update_every=1,\n",
    "                           hypermodel=hypmodel,\n",
    "                           training=False,\n",
    "                           device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(dataset, batch_size=batch_s, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 25, loss:0.200785\n",
      "step 50, loss:0.158958\n",
      "step 75, loss:0.088616\n",
      "step 100, loss:0.107625\n",
      "step 125, loss:0.072764\n",
      "step 150, loss:0.075434\n",
      "step 175, loss:0.064682\n",
      "step 200, loss:0.046084\n",
      "step 225, loss:0.061481\n",
      "step 250, loss:0.035499\n",
      "step 275, loss:0.036420\n",
      "step 300, loss:0.042691\n",
      "step 325, loss:0.058887\n",
      "step 350, loss:0.044830\n",
      "step 375, loss:0.021625\n",
      "step 400, loss:0.048703\n",
      "step 425, loss:0.040008\n",
      "step 450, loss:0.032660\n",
      "step 475, loss:0.048827\n",
      "step 500, loss:0.050698\n",
      "step 525, loss:0.039727\n",
      "step 550, loss:0.031227\n",
      "step 575, loss:0.040123\n",
      "step 600, loss:0.043673\n",
      "step 625, loss:0.028190\n",
      "step 650, loss:0.046981\n",
      "step 675, loss:0.045161\n",
      "step 700, loss:0.052976\n",
      "step 725, loss:0.024599\n",
      "step 750, loss:0.024169\n",
      "step 775, loss:0.041763\n",
      "step 800, loss:0.050895\n",
      "step 825, loss:0.061917\n",
      "step 850, loss:0.031203\n",
      "step 875, loss:0.044208\n",
      "step 900, loss:0.039454\n",
      "step 925, loss:0.025328\n",
      "step 950, loss:0.026930\n",
      "step 975, loss:0.035721\n",
      "step 1000, loss:0.055033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06032814111086119"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_hypermodel(data_loader=dl,\n",
    "                 nsteps=n_steps, \n",
    "                 nsamples_z=n_samples_z,\n",
    "                 learning_rate=lr,\n",
    "                 sigmao=sigmao_training,\n",
    "                 sigmap=sigmap_training,\n",
    "                 hypermodel=hypmodel, device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODOs:\n",
    "- online loop with TS\n",
    "- similar offline tests with the MNL bandit env\n",
    "- script for experiments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
